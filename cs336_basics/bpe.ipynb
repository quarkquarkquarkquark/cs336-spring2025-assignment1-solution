{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc8b3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import BinaryIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077fc23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_chunk_boundaries(\n",
    "    file: BinaryIO, \n",
    "    desired_num_chunks: int, \n",
    "    split_special_token: bytes\n",
    ") -> list[int]:\n",
    "    \"\"\"\n",
    "    Chunk the file into parts that can be counted independently.\n",
    "    May return fewer chunks if the boundaries end up overlapping.\n",
    "    \"\"\"\n",
    "    assert isinstance(split_special_token, bytes), (\n",
    "        \"Must represent special token as a bytestring\"\n",
    "    )\n",
    "\n",
    "    # Get total file size in bytes\n",
    "    file.seek(0, os.SEEK_END)\n",
    "    file_size = file.tell()\n",
    "    file.seek(0)\n",
    "\n",
    "    chunk_size = file_size // desired_num_chunks\n",
    "\n",
    "    # Initial guesses for chunk boundary locations, uniformly spaced\n",
    "    # Chunks start on previous index, don't include last index\n",
    "    chunk_boundaries = [i * chunk_size for i in range(desired_num_chunks + 1)]\n",
    "    chunk_boundaries[-1] = file_size\n",
    "\n",
    "    mini_chunk_size = 4096  # Read ahead by 4k bytes at a time\n",
    "\n",
    "    for bi in range(1, len(chunk_boundaries) - 1):\n",
    "        initial_position = chunk_boundaries[bi]\n",
    "        file.seek(initial_position)  # Start at boundary guess\n",
    "        while True:\n",
    "            mini_chunk = file.read(mini_chunk_size)  # Read a mini chunk\n",
    "\n",
    "            # If EOF, this boundary should be at the end of the file\n",
    "            if mini_chunk == b\"\":\n",
    "                chunk_boundaries[bi] = file_size\n",
    "                break\n",
    "\n",
    "            # Find the special token in the mini chunk\n",
    "            found_at = mini_chunk.find(split_special_token)\n",
    "            if found_at != -1:\n",
    "                chunk_boundaries[bi] = initial_position + found_at\n",
    "                break\n",
    "            initial_position += mini_chunk_size\n",
    "\n",
    "    # Make sure all boundaries are unique, but might be fewer than desired_num_chunks\n",
    "    return sorted(set(chunk_boundaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d8167e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "num_processes=4\n",
    "input_path='/Users/zhanghao1/Downloads/cs336/assignment1-basics/tests/fixtures/tinystories_sample_5M.txt'\n",
    "with open(input_path,\"rb\") as f:\n",
    "    boundaries = find_chunk_boundaries(\n",
    "        f, num_processes, \"<|endoftext|>\".encode(\"utf-8\"))\n",
    "    for start, end in zip(boundaries[:-1], boundaries[1:]):\n",
    "        f.seek(start)\n",
    "        chunk = f.read(end - start).decode(\"utf-8\", errors=\"ignore\")\n",
    "        print(1)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2aaa265",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce100bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens=[\"<|endoftext|>\",\"<|endoftext|>\"]\n",
    "s=\"|\".join([re.escape(item) for item in special_tokens])\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea75009b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "re.split(s,chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5213517e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt=0\n",
    "for i in range(len(chunk)-1):\n",
    "    if chunk[i]=='i' and chunk[i+1]=='n':\n",
    "        cnt+=1\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9336226d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pat=r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "word_cnt={}\n",
    "iterlist=re.finditer(pat,chunk)\n",
    "for match in iterlist:\n",
    "    s=match.group().encode(\"utf-8\")\n",
    "    if s in word_cnt:\n",
    "        word_cnt[s]+=1\n",
    "    else:\n",
    "        word_cnt[s]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0839362d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44551aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file_path = \"/Users/zhanghao1/Downloads/cs336/assignment1-basics/tests/_snapshots/test_train_bpe_special_tokens.pkl\"\n",
    "\n",
    "with open(file_path, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "print(data.keys())\n",
    "reference_merges=data['merges']\n",
    "for idx,item in enumerate(reference_merges):\n",
    "    if b\"\\n\" in item[0] or b'\\n' in item[1]:\n",
    "        print(idx,item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49cf904",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,item in enumerate(data['vocab_values']):\n",
    "    if b'\\n' in item:\n",
    "        print(idx,item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9faed04",
   "metadata": {},
   "outputs": [],
   "source": [
    "ord(b'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de5d24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_pair(lst, pair, new_val):\n",
    "    a, b = pair\n",
    "    res=[]\n",
    "    i=0\n",
    "    while i<len(lst):\n",
    "        print(i)\n",
    "        if i!=len(lst)-1 and lst[i]==pair[0] and lst[i+1]==pair[1]:\n",
    "            res.append(new_val)\n",
    "            i+=2\n",
    "            continue\n",
    "\n",
    "        res.append(lst[i])\n",
    "        i+=1\n",
    "    return res\n",
    "lst=[97,97,97,97,97]+[97,97,97,97,97]\n",
    "pair=(97,97)\n",
    "replace_pair(lst,pair,257)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac6b80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "str_tuples=[(b\"t\",b\"h\"),(b\" c\",b\"om\")]\n",
    "np.lexsort(np.array(str_tuples).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb41242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a855d795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def find_max_pairs_vec(pairs_cnt,vocab):\n",
    "\n",
    "    \n",
    "    max_val = values.max()\n",
    "    max_mask = (values == max_val)\n",
    "    \n",
    "    max_keys = keys[max_mask]\n",
    "    str_tuples = [(vocab[k[1]], vocab[k[0]]) for k in max_keys]\n",
    "    lex_last = np.lexsort(np.array(str_tuples).T)[-1]  # 选字典序最大的\n",
    "    \n",
    "    return tuple(max_keys[lex_last]), max_val\n",
    "\n",
    "def find_max_pairs(pairs_cnt,vocab):\n",
    "    max_cnt=-1\n",
    "    max_pair=(-1,-1)\n",
    "    max_pair_str=None\n",
    "    for item in pairs_cnt.keys():\n",
    "        if pairs_cnt[item]>max_cnt or (pairs_cnt[item]==max_cnt and (vocab[item[0]],vocab[item[1]])>max_pair_str):\n",
    "            max_cnt=pairs_cnt[item]\n",
    "            max_pair=item\n",
    "            max_pair_str=(vocab[item[0]],vocab[item[1]])\n",
    "    return max_pair,max_cnt\n",
    "\n",
    "pairs_cnt={}\n",
    "vocab={}\n",
    "for i in range(5000000):\n",
    "    pairs_cnt[(i,i)]=i\n",
    "    vocab[i]=str(i)\n",
    "t=time.time()\n",
    "for i in range(10):\n",
    "    find_max_pairs(pairs_cnt,vocab)\n",
    "print(time.time()-t)\n",
    "keys = np.array(list(pairs_cnt.keys()))\n",
    "values = np.array(list(pairs_cnt.values()))\n",
    "t=time.time()\n",
    "for i in range(10):\n",
    "    find_max_pairs_vec(pairs_cnt,vocab)\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d04003a",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3209e219",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "dic=Counter()\n",
    "dic[(3,5)]-=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "534ef55c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({(3, 5): -1})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c015c826",
   "metadata": {},
   "outputs": [],
   "source": [
    "del dic[(3,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "265771a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# 从文件加载\n",
    "with open(\"../owt_vocab.pkl\", \"rb\") as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "with open(\"../owt_merges.pkl\", \"rb\") as f:\n",
    "    merges = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98f0c712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<|endoftext|>'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87408958",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=0\n",
    "s=b\"\"\n",
    "max_k=0\n",
    "for k,item in vocab.items():\n",
    "    if len(item)>max_len:\n",
    "        max_len=len(item)\n",
    "        s=item\n",
    "        max_k=k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "050d9cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b' accomplishment' 7160\n"
     ]
    }
   ],
   "source": [
    "print(s,max_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64b40505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([1,3,3,4,3,16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9163468c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n"
     ]
    }
   ],
   "source": [
    "for i in b\"s\":\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea3f728f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b's'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int.to_bytes(115)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e5f08ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa', 'a']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens=['a','aa']\n",
    "special_tokens=sorted(special_tokens, key=len, reverse=True)\n",
    "special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7498fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7577948570251465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12801608.168363236"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "import time\n",
    "reference_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "with open(\"../data/TinyStoriesV2-GPT4-valid.txt\",\"r\") as f:\n",
    "    data=f.read()\n",
    "b_cnt=len(data.encode('utf-8'))\n",
    "t=time.time()\n",
    "reference_tokenizer.encode(data,allowed_special={'<|endoftext|>'})\n",
    "t=time.time()-t\n",
    "print(t)\n",
    "b_cnt/t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "743a84d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_tokenizer.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d49e9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<regex.Match object; span=(14, 14), match=''>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token\n",
    "with open(\"../data/owt_valid.txt\") as f:\n",
    "    for id in f:\n",
    "        break\n",
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f74d2575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from einops import rearrange,einsum\n",
    "d_k=20\n",
    "theta=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3fd66323",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len=12\n",
    "theta_list=1.0 / (theta ** (torch.arange(0, d_k, 2).float() / d_k))  # (dim/2,)\n",
    "t=torch.arange(max_seq_len).unsqueeze(1)*theta_list\n",
    "cos=torch.cos(t)\n",
    "sin=torch.sin(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f50476af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 10, 2])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_t=rearrange(t,\"... (a b) -> ... a b\",a=10,b=2)\n",
    "x_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "54efc5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "even=x_t[...,0]\n",
    "odd=x_t[...,1]\n",
    "out=torch.concat([(cos*even+sin*odd).unsqueeze(-1),(-cos*even+cos*odd).unsqueeze(-1)],axis=-1)\n",
    "out=rearrange(out,\"... a b -> ... (a b)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1cb65d2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 500., 1500., 2500., 3500., 4500., 5500., 6500., 7500., 8500., 9500.])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d5583689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0, d_k, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5cdfffa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[inf, inf, inf],\n",
       "         [inf, inf, inf],\n",
       "         [inf, inf, inf],\n",
       "         [inf, inf, inf],\n",
       "         [inf, inf, inf]],\n",
       "\n",
       "        [[inf, inf, inf],\n",
       "         [inf, inf, inf],\n",
       "         [inf, inf, inf],\n",
       "         [inf, inf, inf],\n",
       "         [inf, inf, inf]]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from einops import einsum\n",
    "x=torch.randn(2,5,3)\n",
    "mask=torch.ones(5,3).to(dtype=torch.bool)\n",
    "x+mask*torch.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ecf2fcbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False, False, False],\n",
       "        [ True,  True, False, False, False],\n",
       "        [ True,  True,  True, False, False],\n",
       "        [ True,  True,  True,  True, False],\n",
       "        [ True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(5,5)).to(dtype=torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "cfe99526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1328755507200, 3019898880000, 164682137600, 4513336524800)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length=1024\n",
    "d_model=1600\n",
    "d_ff=6400\n",
    "vocab_size=50257\n",
    "num_heads=25\n",
    "num_layers=48\n",
    "\n",
    "def cal_flops(\n",
    "    context_length=1024,\n",
    "    d_model=1600,\n",
    "    d_ff=6400,\n",
    "    vocab_size=50257,\n",
    "    num_heads=25,\n",
    "    num_layers=48,\n",
    "):\n",
    "    d_head=d_model//num_heads\n",
    "\n",
    "\n",
    "    mha=3*2*d_model*context_length*d_model\\\n",
    "        +num_heads*2*context_length*d_head*context_length\\\n",
    "        +num_heads*2*context_length*context_length*d_head\\\n",
    "        +2*context_length*d_model*d_model\n",
    "\n",
    "    ffn=6*d_ff*d_model*context_length\n",
    "\n",
    "    lm_head=2*context_length*d_model*vocab_size\n",
    "    total=(mha+ffn)*num_layers+lm_head\n",
    "\n",
    "    return mha*num_layers,ffn*num_layers,lm_head,total\n",
    "\n",
    "cal_flops()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ab77e02f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[98.5694994432, 48.31838208, 2.6349142016, 149.5227957248]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPT-2 small (12 layers, 768d_model, 12 heads), GPT-2 medium (24layers, 1024d_model, 16 heads), and GPT-2 large\n",
    "items=cal_flops(context_length=16384)\n",
    "[item/1e12 for item in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "6b6a2563",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.randn(5,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ebfb9220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6428, -0.0197,  0.3973,  0.2688, -1.0461])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind=torch.arange(5,10,dtype=torch.int64).unsqueeze(1)\n",
    "\n",
    "torch.gather(x,index=ind,dim=1).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662059b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class adamw(torch.optim.Optimizer):\n",
    "    def __init__(self,weights,lr,beta_1,beta_2,lamb,eps=1e-8):\n",
    "        super(weights,lr).__init__()\n",
    "        self.beta_1=beta_1\n",
    "        self.beta_2=beta_2\n",
    "        self.m=[torch.zeros_like(p) for p in weights]\n",
    "        self.v=[torch.zeros_like(p) for p in weights]\n",
    "        self.lamb=lamb\n",
    "        self.eps=eps\n",
    "\n",
    "    def step(self,closure: Optional[Callable]=None):\n",
    "        loss = None if closure is None else closure()\n",
    "        for group in self.param_groups:\n",
    "            lr=group['lr']\n",
    "            print(group.keys())\n",
    "            for p in group[\"params\"]:\n",
    "                print(type(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d78f4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.056364059448242\n",
      "9406.3466796875\n",
      "3395691.0\n",
      "1225844352.0\n",
      "442529775616.0\n",
      "159753254731776.0\n",
      "5.767092247514317e+16\n",
      "2.0819199895380427e+19\n",
      "7.515730907145636e+21\n",
      "2.7131791119329537e+24\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import SGD\n",
    "from torch import nn\n",
    "weights=nn.Parameter(5*torch.randn((10,10)))\n",
    "opt=SGD([weights],lr=1e3)\n",
    "for t in range(10):\n",
    "    opt.zero_grad()\n",
    "    loss=(weights**2).mean()\n",
    "    print(loss.cpu().item())\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "a9ebd19b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 6, 7, 8, 9]])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf38fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
