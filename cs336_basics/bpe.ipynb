{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc8b3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import BinaryIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077fc23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_chunk_boundaries(\n",
    "    file: BinaryIO, \n",
    "    desired_num_chunks: int, \n",
    "    split_special_token: bytes\n",
    ") -> list[int]:\n",
    "    \"\"\"\n",
    "    Chunk the file into parts that can be counted independently.\n",
    "    May return fewer chunks if the boundaries end up overlapping.\n",
    "    \"\"\"\n",
    "    assert isinstance(split_special_token, bytes), (\n",
    "        \"Must represent special token as a bytestring\"\n",
    "    )\n",
    "\n",
    "    # Get total file size in bytes\n",
    "    file.seek(0, os.SEEK_END)\n",
    "    file_size = file.tell()\n",
    "    file.seek(0)\n",
    "\n",
    "    chunk_size = file_size // desired_num_chunks\n",
    "\n",
    "    # Initial guesses for chunk boundary locations, uniformly spaced\n",
    "    # Chunks start on previous index, don't include last index\n",
    "    chunk_boundaries = [i * chunk_size for i in range(desired_num_chunks + 1)]\n",
    "    chunk_boundaries[-1] = file_size\n",
    "\n",
    "    mini_chunk_size = 4096  # Read ahead by 4k bytes at a time\n",
    "\n",
    "    for bi in range(1, len(chunk_boundaries) - 1):\n",
    "        initial_position = chunk_boundaries[bi]\n",
    "        file.seek(initial_position)  # Start at boundary guess\n",
    "        while True:\n",
    "            mini_chunk = file.read(mini_chunk_size)  # Read a mini chunk\n",
    "\n",
    "            # If EOF, this boundary should be at the end of the file\n",
    "            if mini_chunk == b\"\":\n",
    "                chunk_boundaries[bi] = file_size\n",
    "                break\n",
    "\n",
    "            # Find the special token in the mini chunk\n",
    "            found_at = mini_chunk.find(split_special_token)\n",
    "            if found_at != -1:\n",
    "                chunk_boundaries[bi] = initial_position + found_at\n",
    "                break\n",
    "            initial_position += mini_chunk_size\n",
    "\n",
    "    # Make sure all boundaries are unique, but might be fewer than desired_num_chunks\n",
    "    return sorted(set(chunk_boundaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d8167e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "num_processes=4\n",
    "input_path='/Users/zhanghao1/Downloads/cs336/assignment1-basics/tests/fixtures/tinystories_sample_5M.txt'\n",
    "with open(input_path,\"rb\") as f:\n",
    "    boundaries = find_chunk_boundaries(\n",
    "        f, num_processes, \"<|endoftext|>\".encode(\"utf-8\"))\n",
    "    for start, end in zip(boundaries[:-1], boundaries[1:]):\n",
    "        f.seek(start)\n",
    "        chunk = f.read(end - start).decode(\"utf-8\", errors=\"ignore\")\n",
    "        print(1)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2aaa265",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce100bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens=[\"<|endoftext|>\",\"<|endoftext|>\"]\n",
    "s=\"|\".join([re.escape(item) for item in special_tokens])\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea75009b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "re.split(s,chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5213517e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt=0\n",
    "for i in range(len(chunk)-1):\n",
    "    if chunk[i]=='i' and chunk[i+1]=='n':\n",
    "        cnt+=1\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9336226d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pat=r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "word_cnt={}\n",
    "iterlist=re.finditer(pat,chunk)\n",
    "for match in iterlist:\n",
    "    s=match.group().encode(\"utf-8\")\n",
    "    if s in word_cnt:\n",
    "        word_cnt[s]+=1\n",
    "    else:\n",
    "        word_cnt[s]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0839362d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44551aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file_path = \"/Users/zhanghao1/Downloads/cs336/assignment1-basics/tests/_snapshots/test_train_bpe_special_tokens.pkl\"\n",
    "\n",
    "with open(file_path, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "print(data.keys())\n",
    "reference_merges=data['merges']\n",
    "for idx,item in enumerate(reference_merges):\n",
    "    if b\"\\n\" in item[0] or b'\\n' in item[1]:\n",
    "        print(idx,item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49cf904",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,item in enumerate(data['vocab_values']):\n",
    "    if b'\\n' in item:\n",
    "        print(idx,item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9faed04",
   "metadata": {},
   "outputs": [],
   "source": [
    "ord(b'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de5d24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_pair(lst, pair, new_val):\n",
    "    a, b = pair\n",
    "    res=[]\n",
    "    i=0\n",
    "    while i<len(lst):\n",
    "        print(i)\n",
    "        if i!=len(lst)-1 and lst[i]==pair[0] and lst[i+1]==pair[1]:\n",
    "            res.append(new_val)\n",
    "            i+=2\n",
    "            continue\n",
    "\n",
    "        res.append(lst[i])\n",
    "        i+=1\n",
    "    return res\n",
    "lst=[97,97,97,97,97]+[97,97,97,97,97]\n",
    "pair=(97,97)\n",
    "replace_pair(lst,pair,257)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac6b80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "str_tuples=[(b\"t\",b\"h\"),(b\" c\",b\"om\")]\n",
    "np.lexsort(np.array(str_tuples).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb41242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a855d795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def find_max_pairs_vec(pairs_cnt,vocab):\n",
    "\n",
    "    \n",
    "    max_val = values.max()\n",
    "    max_mask = (values == max_val)\n",
    "    \n",
    "    max_keys = keys[max_mask]\n",
    "    str_tuples = [(vocab[k[1]], vocab[k[0]]) for k in max_keys]\n",
    "    lex_last = np.lexsort(np.array(str_tuples).T)[-1]  # 选字典序最大的\n",
    "    \n",
    "    return tuple(max_keys[lex_last]), max_val\n",
    "\n",
    "def find_max_pairs(pairs_cnt,vocab):\n",
    "    max_cnt=-1\n",
    "    max_pair=(-1,-1)\n",
    "    max_pair_str=None\n",
    "    for item in pairs_cnt.keys():\n",
    "        if pairs_cnt[item]>max_cnt or (pairs_cnt[item]==max_cnt and (vocab[item[0]],vocab[item[1]])>max_pair_str):\n",
    "            max_cnt=pairs_cnt[item]\n",
    "            max_pair=item\n",
    "            max_pair_str=(vocab[item[0]],vocab[item[1]])\n",
    "    return max_pair,max_cnt\n",
    "\n",
    "pairs_cnt={}\n",
    "vocab={}\n",
    "for i in range(5000000):\n",
    "    pairs_cnt[(i,i)]=i\n",
    "    vocab[i]=str(i)\n",
    "t=time.time()\n",
    "for i in range(10):\n",
    "    find_max_pairs(pairs_cnt,vocab)\n",
    "print(time.time()-t)\n",
    "keys = np.array(list(pairs_cnt.keys()))\n",
    "values = np.array(list(pairs_cnt.values()))\n",
    "t=time.time()\n",
    "for i in range(10):\n",
    "    find_max_pairs_vec(pairs_cnt,vocab)\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d04003a",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3209e219",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "dic=Counter()\n",
    "dic[(3,5)]-=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "534ef55c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({(3, 5): -1})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c015c826",
   "metadata": {},
   "outputs": [],
   "source": [
    "del dic[(3,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "265771a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# 从文件加载\n",
    "with open(\"../owt_vocab.pkl\", \"rb\") as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "with open(\"../owt_merges.pkl\", \"rb\") as f:\n",
    "    merges = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98f0c712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<|endoftext|>'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87408958",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=0\n",
    "s=b\"\"\n",
    "max_k=0\n",
    "for k,item in vocab.items():\n",
    "    if len(item)>max_len:\n",
    "        max_len=len(item)\n",
    "        s=item\n",
    "        max_k=k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "050d9cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b' accomplishment' 7160\n"
     ]
    }
   ],
   "source": [
    "print(s,max_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64b40505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([1,3,3,4,3,16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9163468c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n"
     ]
    }
   ],
   "source": [
    "for i in b\"s\":\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea3f728f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b's'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int.to_bytes(115)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e5f08ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa', 'a']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens=['a','aa']\n",
    "special_tokens=sorted(special_tokens, key=len, reverse=True)\n",
    "special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7498fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7577948570251465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12801608.168363236"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "import time\n",
    "reference_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "with open(\"../data/TinyStoriesV2-GPT4-valid.txt\",\"r\") as f:\n",
    "    data=f.read()\n",
    "b_cnt=len(data.encode('utf-8'))\n",
    "t=time.time()\n",
    "reference_tokenizer.encode(data,allowed_special={'<|endoftext|>'})\n",
    "t=time.time()-t\n",
    "print(t)\n",
    "b_cnt/t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "743a84d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_tokenizer.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d49e9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<regex.Match object; span=(14, 14), match=''>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token\n",
    "with open(\"../data/owt_valid.txt\") as f:\n",
    "    for id in f:\n",
    "        break\n",
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f74d2575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from einops import rearrange,einsum\n",
    "d_k=20\n",
    "theta=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3fd66323",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len=12\n",
    "theta_list=1.0 / (theta ** (torch.arange(0, d_k, 2).float() / d_k))  # (dim/2,)\n",
    "t=torch.arange(max_seq_len).unsqueeze(1)*theta_list\n",
    "cos=torch.cos(t)\n",
    "sin=torch.sin(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f50476af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 10, 2])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_t=rearrange(t,\"... (a b) -> ... a b\",a=10,b=2)\n",
    "x_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "54efc5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "even=x_t[...,0]\n",
    "odd=x_t[...,1]\n",
    "out=torch.concat([(cos*even+sin*odd).unsqueeze(-1),(-cos*even+cos*odd).unsqueeze(-1)],axis=-1)\n",
    "out=rearrange(out,\"... a b -> ... (a b)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1cb65d2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 500., 1500., 2500., 3500., 4500., 5500., 6500., 7500., 8500., 9500.])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d5583689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [1.0000e+00, 3.9811e-01, 1.5849e-01, 6.3096e-02, 2.5119e-02, 1.0000e-02,\n",
       "         3.9811e-03, 1.5849e-03, 6.3096e-04, 2.5119e-04],\n",
       "        [2.0000e+00, 7.9621e-01, 3.1698e-01, 1.2619e-01, 5.0238e-02, 2.0000e-02,\n",
       "         7.9621e-03, 3.1698e-03, 1.2619e-03, 5.0238e-04],\n",
       "        [3.0000e+00, 1.1943e+00, 4.7547e-01, 1.8929e-01, 7.5357e-02, 3.0000e-02,\n",
       "         1.1943e-02, 4.7547e-03, 1.8929e-03, 7.5357e-04],\n",
       "        [4.0000e+00, 1.5924e+00, 6.3396e-01, 2.5238e-01, 1.0048e-01, 4.0000e-02,\n",
       "         1.5924e-02, 6.3396e-03, 2.5238e-03, 1.0048e-03],\n",
       "        [5.0000e+00, 1.9905e+00, 7.9245e-01, 3.1548e-01, 1.2559e-01, 5.0000e-02,\n",
       "         1.9905e-02, 7.9245e-03, 3.1548e-03, 1.2559e-03],\n",
       "        [6.0000e+00, 2.3886e+00, 9.5094e-01, 3.7857e-01, 1.5071e-01, 6.0000e-02,\n",
       "         2.3886e-02, 9.5094e-03, 3.7857e-03, 1.5071e-03],\n",
       "        [7.0000e+00, 2.7868e+00, 1.1094e+00, 4.4167e-01, 1.7583e-01, 7.0000e-02,\n",
       "         2.7867e-02, 1.1094e-02, 4.4167e-03, 1.7583e-03],\n",
       "        [8.0000e+00, 3.1849e+00, 1.2679e+00, 5.0477e-01, 2.0095e-01, 8.0000e-02,\n",
       "         3.1849e-02, 1.2679e-02, 5.0477e-03, 2.0095e-03],\n",
       "        [9.0000e+00, 3.5830e+00, 1.4264e+00, 5.6786e-01, 2.2607e-01, 9.0000e-02,\n",
       "         3.5830e-02, 1.4264e-02, 5.6786e-03, 2.2607e-03],\n",
       "        [1.0000e+01, 3.9811e+00, 1.5849e+00, 6.3096e-01, 2.5119e-01, 1.0000e-01,\n",
       "         3.9811e-02, 1.5849e-02, 6.3096e-03, 2.5119e-03],\n",
       "        [1.1000e+01, 4.3792e+00, 1.7434e+00, 6.9405e-01, 2.7631e-01, 1.1000e-01,\n",
       "         4.3792e-02, 1.7434e-02, 6.9405e-03, 2.7631e-03]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdfffa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
